{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219c1b63",
   "metadata": {
    "id": "219c1b63"
   },
   "source": [
    "# Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f989ed08",
   "metadata": {
    "id": "f989ed08"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453e8c0",
   "metadata": {
    "id": "9453e8c0"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dfa47",
   "metadata": {
    "id": "4d7dfa47"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/Users/chathurya/Downloads/heartfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3989b9",
   "metadata": {
    "id": "eb3989b9"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0610d2c",
   "metadata": {
    "id": "b0610d2c"
   },
   "source": [
    "<span style='font-size:20px; color: blue'><b>Data Visualization and Cleaning</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb186d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bacb186d",
    "outputId": "9662a1c8-9f55-48c7-b104-9eba4991a014"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d06d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "433d06d5",
    "outputId": "17f07710-72b8-4588-c7b8-0ab8af515995"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8774207",
   "metadata": {
    "id": "a8774207"
   },
   "source": [
    "<span style='font-size:20px'>Checking for Missing Values</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e2303",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b8e2303",
    "outputId": "a73bd944-b0bd-446c-8e1d-0cd89b37a39d"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ba830",
   "metadata": {
    "id": "3a2ba830"
   },
   "source": [
    "<span style='font-size:20px'>Checking for Duplicated Rows</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36da308",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e36da308",
    "outputId": "b8c72e0a-2b16-4430-cb3a-0e08fa96b839"
   },
   "outputs": [],
   "source": [
    "# Count the number of duplicated rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "\n",
    "# Print the number of duplicated rows\n",
    "print(f\"Number of duplicated rows: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee786e",
   "metadata": {
    "id": "c6ee786e"
   },
   "source": [
    "<span style='font-size:20px'>Handle duplicated rows</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf02f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbaf02f2",
    "outputId": "be94ea13-7212-45f6-cef5-b3053a2e09c2"
   },
   "outputs": [],
   "source": [
    "# Keep only the first occurrence of duplicated rows\n",
    "df = df.drop_duplicates()\n",
    "#confirm if data duplication have been handled\n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ff155",
   "metadata": {
    "id": "073ff155"
   },
   "source": [
    "<span style='font-size:20px'>**Correlation**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZRULfX_q3QDo",
   "metadata": {
    "id": "ZRULfX_q3QDo"
   },
   "source": [
    "useful for understanding the relationships between different variables in your dataset. It helps in identifying highly correlated variables, which can have implications for feature selection and gaining insights into the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7606b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "4f7606b9",
    "outputId": "451069f7-f378-4082-f7c7-bea380c8cd02"
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation coefficient matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-9_6_6ME3ieM",
   "metadata": {
    "id": "-9_6_6ME3ieM"
   },
   "source": [
    "\n",
    "*   1 indicates a perfect positive linear relationship (both variables increase together).\n",
    "*   -1 indicates a perfect negative linear relationship (one variable increases while the other decreases together).\n",
    "*   0 indicates no linear relationship (the variables are not correlated).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97439f8a",
   "metadata": {
    "id": "97439f8a"
   },
   "source": [
    "RESULTS OF THE PREDICTED MODEL BEFORE REMOVING\n",
    "EXTREME OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd56fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "id": "e5cd56fe",
    "outputId": "44f1b07d-9702-4cf1-e67b-2f9d0bbb7a11"
   },
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Initialize the model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print the precision\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Print the recall\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 Score:\", f1)\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28d63e",
   "metadata": {
    "id": "0a28d63e"
   },
   "source": [
    "<span style='font-size:20px; color:blue'><b>Checking For Outliers</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hEyo1HFu4JT3",
   "metadata": {
    "id": "hEyo1HFu4JT3"
   },
   "source": [
    "performing outlier detection and removal using the Interquartile Range (IQR) method.\n",
    "Data points that fall above the upper threshold or below the lower threshold are considered extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92551b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b92551b",
    "outputId": "220ff7f0-145d-4ab2-edfd-17ea16935393"
   },
   "outputs": [],
   "source": [
    "# Calculate the IQR for each column\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Determine the threshold for extreme outliers\n",
    "upper_threshold = Q3 + 3 * IQR\n",
    "lower_threshold = Q1 - 3 * IQR\n",
    "\n",
    "# Remove extreme outliers\n",
    "df_filtered = df[((df >= lower_threshold) & (df <= upper_threshold)).all(axis=1)]\n",
    "\n",
    "# Calculate the number of removed instances\n",
    "removed_instances = len(df) - len(df_filtered)\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(df_filtered)\n",
    "print()\n",
    "\n",
    "# Print the number of removed instances\n",
    "print(f\"Number of removed instances: {removed_instances}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16628810",
   "metadata": {
    "id": "16628810"
   },
   "source": [
    "RESULTS OF THE PREDICTED MODEL AFTER REMOVING\n",
    "EXTREME OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0f38c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "id": "25b0f38c",
    "outputId": "994ea39b-c6b6-432c-b8ff-702d9706b8e1"
   },
   "outputs": [],
   "source": [
    "X = df_filtered .drop('target', axis=1)\n",
    "y = df_filtered ['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Initialize the model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print the precision\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Print the recall\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb099c",
   "metadata": {
    "id": "61cb099c"
   },
   "source": [
    "<span style='font-size:20px; color: blue'><b>Checking for Imbalances</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iPs9EwedtnKn",
   "metadata": {
    "id": "iPs9EwedtnKn"
   },
   "source": [
    "imbalanced data can introduce bias into the predictive models\n",
    "When one class is significantly more prevalent than the other, the model may become biased towards the majority class, leading to poor performance in predicting the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd066c47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "dd066c47",
    "outputId": "24481f63-8bfa-41d9-f12e-bea4bf1b3a30"
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each category\n",
    "category_counts = df_filtered ['target'].value_counts()\n",
    "\n",
    "# Calculate the balance ratio\n",
    "balance_ratio = category_counts[0] / category_counts[1]\n",
    "\n",
    "# Plot the category counts\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df, x='target')\n",
    "plt.title(\"Output Balance\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Print the balance ratio\n",
    "print(f\"Balance ratio: {balance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OjnBgWfNu49C",
   "metadata": {
    "id": "OjnBgWfNu49C"
   },
   "source": [
    "majority class has approximately 0.83 times more samples than the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d7078",
   "metadata": {
    "id": "d05d7078"
   },
   "source": [
    "<span style='font-size:20px'>**Data Resampling**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFT3PfjovUGa",
   "metadata": {
    "id": "oFT3PfjovUGa"
   },
   "source": [
    "used to handle imbalanced datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf9226",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96bf9226",
    "outputId": "cd0dd347-4acb-4742-c0a3-8a4a53e09c8c"
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each category\n",
    "category_counts = df['target'].value_counts()\n",
    "\n",
    "# Calculate the original balance ratio\n",
    "original_balance_ratio = category_counts[0] / category_counts[1]\n",
    "\n",
    "# Resample the data to balance the classes\n",
    "resampled_data = df.groupby('target', as_index=False).apply(lambda x: x.sample(n=category_counts.min(), replace=True)).reset_index(drop=True)\n",
    "\n",
    "# Count the occurrences of each category in the resampled data\n",
    "resampled_category_counts = resampled_data['target'].value_counts()\n",
    "\n",
    "# Calculate the resampled balance ratio\n",
    "resampled_balance_ratio = resampled_category_counts[0] / resampled_category_counts[1]\n",
    "\n",
    "# Print the resampled balance ratio\n",
    "print(f\"Resampled Balance ratio: {resampled_balance_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d913be",
   "metadata": {
    "id": "b9d913be"
   },
   "source": [
    "<span style='font-size:20px; color:blue'><b>Feature selectiong</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y1p93dVoyEsd",
   "metadata": {
    "id": "Y1p93dVoyEsd"
   },
   "source": [
    "RFECV is a feature selection method that recursively removes less important features and selects the most important ones based on cross-validated performance.\n",
    "StratifiedKFold ensures that each fold in the cross-validation retains the same class distribution as the original dataset, which is essential when dealing with imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce8f07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8ce8f07",
    "outputId": "31839635-81b6-4b0e-b641-97bcd15a2514"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Separate features and target variable\n",
    "X = resampled_data.drop('target', axis=1)\n",
    "y = resampled_data['target']\n",
    "\n",
    "# Initialize the model for feature selection\n",
    "estimator = GradientBoostingClassifier()\n",
    "\n",
    "# Initialize RFECV with the estimator and cross-validation strategy\n",
    "rfecv = RFECV(estimator=estimator, cv=StratifiedKFold())\n",
    "\n",
    "# Fit RFECV on the data\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "# Get the optimal number of features\n",
    "optimal_num_features = rfecv.n_features_\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[rfecv.support_]\n",
    "\n",
    "# Print the optimal number of features\n",
    "print(\"Optimal Number of Features:\", optimal_num_features)\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ce73b",
   "metadata": {
    "id": "1a7ce73b"
   },
   "outputs": [],
   "source": [
    "X=resampled_data[['cp', 'chol', 'thalach', 'oldpeak', 'ca', 'thal']]\n",
    "y=resampled_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b6af8",
   "metadata": {
    "id": "ee6b6af8"
   },
   "source": [
    "<span style='font-size:20px; color:blue'><b>Data Splitting</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da29eca",
   "metadata": {
    "id": "4da29eca"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bb7d7",
   "metadata": {
    "id": "2f1bb7d7"
   },
   "source": [
    "# Applied Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f73d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "416f73d6",
    "outputId": "cecc8152-cd1b-4c09-fa28-97bb87839096"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef923a9",
   "metadata": {
    "id": "3ef923a9"
   },
   "source": [
    "RESULTS OF THE PREDICTED MODEL AFTER APPLYING\n",
    "FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b5172",
   "metadata": {
    "id": "d04b5172"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af2b5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "id": "75af2b5f",
    "outputId": "dde34b97-c8b6-462b-b762-d65dba6ea187"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print the precision\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Print the recall\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 Score:\", f1)\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7M1vgCx60bZV",
   "metadata": {
    "id": "7M1vgCx60bZV"
   },
   "source": [
    "confusion matrix:\n",
    "*  25 instances were correctly predicted as Class 0 (true negatives).\n",
    "*   5 instances were incorrectly predicted as Class 1, but they actually belong to Class 0 (false positives or Type I errors).\n",
    "*   2 instances were incorrectly predicted as Class 0, but they actually belong to Class 1 (false negatives or Type II errors).\n",
    "*  24 instances were correctly predicted as Class 1 (true positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B6LqWXEE0472",
   "metadata": {
    "id": "B6LqWXEE0472"
   },
   "source": [
    "Accuracy measures the proportion of correctly classified instances out of the total instances in the dataset.\n",
    "*   model achieved an accuracy of 0.875, which means approximately 87.5% of the instances were correctly classified.\n",
    "\n",
    "\n",
    ">\n",
    "\n",
    "\n",
    "Precision, also known as positive predictive value, measures the proportion of true positive predictions out of all positive predictions made by the model.\n",
    "*   model achieved a precision of approximately 82.76%, indicating that when it predicted an instance to belong to the positive class, it was correct about 82.76% of the time.\n",
    "\n",
    "\n",
    ">\n",
    "\n",
    "\n",
    "Recall, also known as sensitivity or true positive rate, measures the proportion of true positive predictions out of all actual positive instances in the dataset.\n",
    "*   model achieved a recall of approximately 92.31%, indicating that it correctly identified 92.31% of the actual positive instances in the dataset.\n",
    "\n",
    "\n",
    ">\n",
    "\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall and provides a balanced measure of the model's performance that takes both false positives and false negatives into account.\n",
    "*   F1 score is approximately 87.27%, indicating a good balance between precision and recall.\n",
    "\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
